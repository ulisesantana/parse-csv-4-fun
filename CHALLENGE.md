# Parse CSV File with Node.js Streams - Backend Exercise

ðŸ§ª Realistic Backend Exercise â€“ Node.js Streams

ðŸ§© Scenario:
Youâ€™re building an internal tool that allows users to upload very large CSV files (over 2GB) containing customer data.

Your task is to:
â€¢	Stream the CSV file line by line.
â€¢	Process each line (validate and transform the data).
â€¢	Write the processed output to a new file.
â€¢	The solution must not load the entire file into memory at any point.

â¸»

âœ… Requirements:
1.	Read an input file named input.csv.
2.	For each line:
â€¢	Split the values (name, email, age).
â€¢	Validate that the email contains @ and that age is a valid number.
â€¢	Convert the name to uppercase.
3.	Write each valid line to output.csv.
4.	Skip lines that are invalid (either due to malformed structure or failed validation).
5.	You may only use built-in Node.js modules (fs, stream, readline, etc.).

â¸»

ðŸ”§ Hints:
â€¢	Use fs.createReadStream to read the file as a stream.
â€¢	Use readline.createInterface to handle line-by-line reading.
â€¢	Use fs.createWriteStream to write the transformed output.
â€¢	Use String.prototype.split and basic validation with regular expressions or Number() to parse and check the data.
